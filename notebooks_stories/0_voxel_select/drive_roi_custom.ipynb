{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "import imodelsx.util\n",
    "import sasc.viz\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "import img2pdf\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import norm\n",
    "from math import ceil\n",
    "from sasc.config import CACHE_DIR, RESULTS_DIR, cache_ngrams_dir, regions_idxs_dir, FMRI_DIR\n",
    "import sasc.modules.fmri_module\n",
    "ngrams_list = joblib.load(join(cache_ngrams_dir, 'fmri_UTS02_ngrams.pkl')) # ngrams are same for both models\n",
    "\n",
    "subject = 'S01'\n",
    "# subject = 'S03'\n",
    "# subject = 'S02'\n",
    "# suffix_setting = '_fedorenko'\n",
    "suffix_setting = '_spotlights'\n",
    "\n",
    "if suffix_setting == '':\n",
    "    # rois_dict = joblib.load(join(regions_idxs_dir, f'rois_{subject}.jbl'))\n",
    "    # rois = joblib.load(join(FMRI_DIR, 'brain_tune/voxel_neighbors_and_pcs/', 'communication_rois_UTS02.jbl'))\n",
    "    rois = joblib.load(join(FMRI_DIR, 'brain_tune/voxel_neighbors_and_pcs/',\n",
    "                            f'communication_rois_v2_UT{subject}.jbl'))\n",
    "    rois_dict_raw = {i: rois[i] for i in range(len(rois))}\n",
    "    if subject == 'S02':\n",
    "        raw_idxs = [\n",
    "            [0, 7],\n",
    "            [3, 4],\n",
    "            [1, 5],\n",
    "            [2, 6],\n",
    "        ]\n",
    "    elif subject == 'S03':\n",
    "        raw_idxs = [\n",
    "            [0, 7],\n",
    "            [3, 4],\n",
    "            [2, 5],\n",
    "            [1, 6],\n",
    "        ]\n",
    "    rois_dict = {\n",
    "        i: np.vstack([rois_dict_raw[j] for j in idxs]).sum(axis=0)\n",
    "        for i, idxs in enumerate(raw_idxs)\n",
    "    }\n",
    "elif suffix_setting == '_fedorenko':\n",
    "    if subject == 'S03':\n",
    "        rois_fedorenko = joblib.load(join(FMRI_DIR, 'brain_tune/voxel_neighbors_and_pcs/', 'lang_localizer_UTS03.jbl'))\n",
    "    rois_dict = {\n",
    "        i: rois_fedorenko[i] for i in range(len(rois_fedorenko))\n",
    "    }\n",
    "    # rois_dict = rois_dict_raw\n",
    "elif suffix_setting == '_spotlights':\n",
    "    rois_spotlights = joblib.load(f'all_spotlights_UT{subject}.jbl')\n",
    "    rois_dict = {i: rois_spotlights[i][-1] for i in range(len(rois_spotlights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize ROIs\n",
    "Make sure these actually look right (esp. with contralateral regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save pcs\n",
    "n_rois = min(8, len(rois_dict))\n",
    "for i in tqdm(range(n_rois)):\n",
    "    flatmap = rois_dict[i]\n",
    "    # flatmap = np.zeros(weights_arr_full.shape[1])\n",
    "    # flatmap[pfc] = pc_coefs_per_voxel[:, pc_num]\n",
    "    sasc.viz._save_flatmap(\n",
    "        flatmap, subject, fname_save=f'rois_custom_images/communication_{i}_{subject}.png')\n",
    "\n",
    "# read all plots and save as subplots on the same page\n",
    "C = 2\n",
    "R = ceil(n_rois/C)\n",
    "fig, axs = plt.subplots(R, C, figsize=(C * 4, R * 2))\n",
    "axs = axs.ravel()\n",
    "for i in range(n_rois):\n",
    "    axs[i].imshow(Image.open(\n",
    "        f'rois_custom_images/communication_{i}_{subject}.png'))\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f'ROI {i}')\n",
    "plt.savefig(f'communication_subplots_{subject}{suffix_setting}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions from embs\n",
    "Run roi_custom_save_top_ngrams.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1098/1098 [00:25<00:00, 42.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def return_ranked_df(outputs_dict, rois_dict):\n",
    "    outputs_dict_rank = {}\n",
    "    for i, k in tqdm(enumerate(outputs_dict)):\n",
    "        outputs_by_vox = np.vstack(outputs_dict[k])  # n_voxels x n_ngrams\n",
    "        roi_mean = outputs_by_vox.mean(axis=0)\n",
    "        roi_mean_other = np.mean([\n",
    "            np.vstack([outputs_dict[j]])[0].mean(axis=0)\n",
    "            for j in rois_dict.keys() if j != i\n",
    "        ])\n",
    "\n",
    "        # standard approach: use avg response by voxel\n",
    "        # outputs_dict_rank[k] = roi_mean\n",
    "\n",
    "        # use ranks by voxel rather than avg response by voxel\n",
    "        # outputs_dict_rank[k] = np.argsort(outputs_by_vox, axis=1).mean(\n",
    "        #     axis=0) * -1  # / outputs_by_vox.shape[1]\n",
    "\n",
    "        # just output voxels no ranking\n",
    "        vox_idxs = np.where(rois_dict[i])[0]\n",
    "        for i in range(len(outputs_dict[k])):\n",
    "            # voxel only\n",
    "            # outputs_dict_rank[f'ROI{k}_vox{vox_idxs[i]}'] = outputs_dict[k][i]\n",
    "\n",
    "            # subtract mean of roi from the individual voxel\n",
    "            # outputs_dict_rank[f'ROI{k}_vox{vox_idxs[i]}'] = outputs_dict[k][i] - \\\n",
    "            # roi_mean\n",
    "\n",
    "            # subtract mean of all other voxels in roi from the individual voxel\n",
    "            outputs_dict_rank[f'ROI{k}_vox{vox_idxs[i]}'] = outputs_dict[k][i] - \\\n",
    "                roi_mean_other\n",
    "\n",
    "    df = pd.DataFrame(outputs_dict_rank, index=ngrams_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "rank_individual_voxels = False\n",
    "if rank_individual_voxels:\n",
    "    outputs_dict = joblib.load(\n",
    "        join(cache_ngrams_dir, f'rois_communication_ngram_outputs_dict_voxels_{subject}{suffix_setting}_opt.pkl'))\n",
    "    df_opt = return_ranked_df(outputs_dict, rois_dict)\n",
    "    outputs_dict = joblib.load(\n",
    "        join(cache_ngrams_dir, f'rois_communication_ngram_outputs_dict_voxels_{subject}{suffix_setting}_llama.pkl'))\n",
    "    df_llama = return_ranked_df(outputs_dict, rois_dict)\n",
    "    df = df_opt + df_llama\n",
    "else:\n",
    "    outputs_dict = joblib.load(\n",
    "        join(cache_ngrams_dir, f'rois_communication_ngram_outputs_dict_{subject}{suffix_setting}_opt.pkl'))\n",
    "    df_opt = pd.DataFrame(outputs_dict, index=ngrams_list)\n",
    "    outputs_dict = joblib.load(\n",
    "        join(cache_ngrams_dir, f'rois_communication_ngram_outputs_dict_{subject}{suffix_setting}_llama.pkl'))\n",
    "    df_llama = pd.DataFrame(outputs_dict, index=ngrams_list)\n",
    "\n",
    "    # add _only keys\n",
    "    ROI_NAMES = rois_dict.keys()\n",
    "\n",
    "    # precompute these for speed\n",
    "    df_opt_mean = df_opt.mean(axis=1)\n",
    "    df_llama_mean = df_llama.mean(axis=1)\n",
    "    num_rois = len(ROI_NAMES)\n",
    "    for k in tqdm(ROI_NAMES):\n",
    "        k_only = str(k) + '_only'\n",
    "        if suffix_setting == '_spotlights':\n",
    "            df_opt[k_only] = df_opt[k] * (1 + 1/num_rois) - df_opt_mean\n",
    "            df_llama[k_only] = df_llama[k] * (1 + 1/num_rois) - df_llama_mean\n",
    "        else:\n",
    "            df_opt[k_only] = df_opt[k] - \\\n",
    "                df_opt[[k for k in ROI_NAMES if k != k]].mean(axis=1)\n",
    "            df_llama[k_only] = df_llama[k] - \\\n",
    "                df_llama[[k for k in ROI_NAMES if k != k]].mean(axis=1)\n",
    "        # df[k_only] = df_opt[k] - \\\n",
    "        # df[[c for c in ROI_NAMES if c != k]].mean(axis=1)\n",
    "    df = df_opt + df_llama\n",
    "\n",
    "    # replace df values with ranks (here, higher is better, both before and after)\n",
    "    df = df.rank(axis=0)\n",
    "\n",
    "stability_scores = {\n",
    "    k: np.corrcoef(df_opt[k], df_llama[k])[0, 1]\n",
    "    for k in df.columns\n",
    "}\n",
    "\n",
    "# get top ngrams (highest scores/ranks) for each ROI\n",
    "top_ngrams_dict = {}\n",
    "for k in tqdm(df.columns):\n",
    "    top_ngrams_dict[k] = df[k].sort_values(\n",
    "        ascending=False).index[:100].tolist()\n",
    "top_ngrams_df = pd.DataFrame(top_ngrams_dict)\n",
    "top_ngrams_df.to_csv(f'top_ngrams_by_roi_{subject}{suffix_setting}.csv')\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "# rois = rois_dict.keys()\n",
    "# rois = [r for r in rois if not r == 'pSTS']  # never localized pSTS in S03\n",
    "# display(top_ngrams_df.head(30))\n",
    "top_ngrams_df.to_pickle(\n",
    "    f'top_ngrams_custom_communication_{subject}{suffix_setting}.pkl')\n",
    "joblib.dump(stability_scores,\n",
    "            f'stability_scores_{subject}{suffix_setting}.jbl')\n",
    "\n",
    "# top_ngrams_df = pd.read_pickle(\n",
    "# f'top_ngrams_custom_communication_{subject}{suffix_setting}.pkl')\n",
    "\n",
    "if suffix_setting == '_spotlights':\n",
    "   # filter based on stability score\n",
    "    STABILITY_THRESH = 0.6\n",
    "    ks = [k for k in top_ngrams_df.columns if stability_scores[k] > STABILITY_THRESH]\n",
    "    # ks = [k for k in ks if 'only' not in str(k)]\n",
    "    ks = [k for k in ks if 'only' in str(k)]\n",
    "    print(len(ks))\n",
    "\n",
    "    top_ngrams_df_filt = top_ngrams_df[ks]\n",
    "    top_ngrams_df_filt.to_pickle(\n",
    "        f'top_ngrams_custom_communication_{subject}{suffix_setting}_filtered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run openai_calls script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt4 = imodelsx.llm.get_llm('gpt-4-turbo-0125-spot')\n",
    "# try:\n",
    "#     explanations = json.load(open(f'explanations_by_roi_{subject}.json', 'r'))\n",
    "# except:\n",
    "#     explanations = {}\n",
    "# for k in top_ngrams_df.columns:\n",
    "\n",
    "#     s = '- ' + '\\n- '.join(top_ngrams_df[k].iloc[:60])\n",
    "#     prompt = f'''Here is a list of phrases:\n",
    "#     {s}\n",
    "\n",
    "#     What is a common theme among these phrases? Return only a concise phrase.'''\n",
    "#     if not k in explanations:\n",
    "#         explanations[k] = gpt4(prompt)\n",
    "# # json.dump(explanations, open(\n",
    "#     # f'explanations_by_roi_communication_{subject}.json', 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 3)\n"
     ]
    }
   ],
   "source": [
    "# subject = 'S03'\n",
    "# subject = 'S02'\n",
    "subject = 'S01'\n",
    "\n",
    "explanations = json.load(\n",
    "    open(f'explanations_by_roi_communication_{subject}{suffix_setting}.json', 'r'))\n",
    "stability_scores = joblib.load(\n",
    "    f'stability_scores_{subject}{suffix_setting}.jbl')\n",
    "top_ngrams_df = pd.read_pickle(\n",
    "    f'top_ngrams_custom_communication_{subject}{suffix_setting}_filtered.pkl')\n",
    "\n",
    "\n",
    "# dataframe of explanations and stability scores\n",
    "explanations_df = pd.DataFrame(explanations, index=['explanation']).T\n",
    "explanations_df['explanation'] = explanations_df['explanation'].str.lower().str.replace(\n",
    "    r'[^\\w\\s]', '').str.replace(r'\\s+', ' ').str.replace('.', '').str.strip()\n",
    "stab = pd.Series(stability_scores)\n",
    "# set index to al lbe strings\n",
    "stab.index = stab.index.astype(str)\n",
    "explanations_df['stability'] = stab\n",
    "\n",
    "# add top ngrams\n",
    "top_ngrams_list_list = top_ngrams_df.T.apply(lambda x: x.tolist(), axis=1)\n",
    "top_ngrams_list_list.index = top_ngrams_list_list.index.astype(str)\n",
    "explanations_df['top_ngrams'] = top_ngrams_list_list\n",
    "explanations_df = explanations_df.sort_values('explanation', ascending=False)\n",
    "explanations_df.to_csv(\n",
    "    f'communication_explanations_{subject}{suffix_setting}.csv')\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "# display(explanations_df)\n",
    "print(explanations_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase, remove punctuation, trim\n",
    "explanations_df['explanation'].value_counts().head(50)\n",
    "# split index on _\n",
    "explanations_df['roi'] = explanations_df.index.str.split('_').str[0]\n",
    "# groupby roi and show most common explanations\n",
    "# display all\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(explanations_df.groupby('roi')[\n",
    "            'explanation'].value_counts().groupby('roi').head(5))\n",
    "# explanations_df.groupby('roi')['explanation'].value_counts().groupby('roi').head(50)\n",
    "# explanations_df.to_csv(f'communication_explanations_{subject}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at comonalities between S02 and S03\n",
    "explanations_dict = {}\n",
    "for subj in ['S02', 'S03']:\n",
    "    explanations_dict[subj] = pd.read_csv(\n",
    "        f'communication_explanations_{subj}{suffix_setting}.csv', index_col=0)\n",
    "    print('shape', explanations_dict[subj].shape)\n",
    "explanations_common = set(explanations_dict['S02']['explanation']).intersection(\n",
    "    set(explanations_dict['S03']['explanation']))\n",
    "explanations_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S02 Export selected rois to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'S02'\n",
    "explanations_clean = {\n",
    "    # numeric\n",
    "    'Numbers_only': 'Numbers',\n",
    "    'Time_only': 'Times',\n",
    "    'Years_only': 'Years',\n",
    "    'Measurements_only': 'Measurements',\n",
    "\n",
    "    # communication\n",
    "    'Relationships_only': 'Relationships',\n",
    "    'Dialogue_only':  'Dialogue',\n",
    "    'Introspection_only':  'Introspection',\n",
    "\n",
    "    # extra ROIs\n",
    "    'Gruesome': 'Gruesome body imagery',\n",
    "    'Clothing and Physical Appearance': 'Clothing and Physical Appearance',\n",
    "    'Colors': 'Colors',\n",
    "\n",
    "    'Sexual_and_Romantic_Interactions': 'Sexual and Romantic Interactions',\n",
    "    'Secretive_Or_Covert_Actions': 'Secretive Or Covert Actions',\n",
    "    'Recognition': 'Recognition',\n",
    "\n",
    "    'Positive Emotional Reactions': 'Positive Emotional Reactions',\n",
    "    'Negative Emotional Reactions': 'Negative Emotional Reactions',\n",
    "    'Professions': 'Professions and Personal Backgrounds',\n",
    "    'Fear and Avoidance': \"Fear and Avoidance\"\n",
    "\n",
    "}\n",
    "explanation_avoid_suffixes = {\n",
    "    'Numbers_only': ' Avoid mentioning any measurements or times.',\n",
    "    'Time_only': ' Avoid mentioning any numbers, measurements, or years. ',\n",
    "    'Years_only': ' Avoid mentioning any numbers, measurements, or times of day. (like \"three o\\'clock\" or \"hours\" or \"fifty\")',\n",
    "    'Measurements_only': ' Avoid mentioning any numbers or times. (like \"three o\\'clock\" or \"hours\" or \"fifty\")',\n",
    "\n",
    "    'Relationships_only': ' Avoid mentioning conversations or dialogue interactions between people.',\n",
    "    'Dialogue_only': ' Avoid mentioning the people involved in the conversation.',\n",
    "    'Introspection_only': 'Avoid mentioning conversations or dialogue interactions between people.',\n",
    "\n",
    "    'Gruesome': '',\n",
    "    'Clothing and Physical Appearance': '',\n",
    "    'Colors': '',\n",
    "\n",
    "    'Sexual_and_Romantic_Interactions': '',\n",
    "    'Secretive_Or_Covert_Actions': '',\n",
    "    'Recognition': '',\n",
    "\n",
    "    'Positive Emotional Reactions': '',\n",
    "    'Negative Emotional Reactions': '',\n",
    "    'Professions': '',\n",
    "    'Fear and Avoidance': ''\n",
    "}\n",
    "\n",
    "top_ngrams_clean = {\n",
    "    'Numbers_only': ['five', 'twenty', 'three hundred', 'a million', 'forty six', 'ninety one', 'four', 'eight'],\n",
    "    'Time_only': ['one o\\'clock', 'hours and hours', 'pocketwatch ticked', 'hourglass', 'from dusk till dawn', 'he was early', 'she was late', 'months later', 'minutes later', 'seconds later', 'just before'],\n",
    "    'Years_only': ['of nineteen sixty', 'until nineteen sixty', 'until nineteen seventy', 'of nineteen ninety'],\n",
    "    'Measurements_only': ['two mile thick', 'eighty milligrams', 'two hundred gallons', 'several hundred cubic', 'eight thousand square', 'to fifty feet'],\n",
    "\n",
    "    'Relationships_only': ['brother and sister', 'he was a good friend', 'a wonderful mother', 'just like her father', 'boyfriend', 'wife', 'aunt'],\n",
    "    'Dialogue_only': ['he said', 'she said', 'they whispered', 'he chimed in', '\"wow!\"', '\"hello\"', 'he shouted', 'she responded'],\n",
    "    'Introspection_only': ['he reflected', 'she thought back', 'they thought hard', \"hm that's interesting\", 'ah yes', 'thought carefully', 'full of introspection'],\n",
    "\n",
    "    'Gruesome': ['my scalp peeled', 'soaked with sweat', 'eyes were swollen', 'of skin peeled', 'legs were swollen', 'swollen and red', 'burned the skin', 'blood was trickling', 'skin was bubbling'],\n",
    "    'Clothing and Physical Appearance': ['in his beret', 'in a bathrobe', 'a red hoodie', 'blonde hair dangling', 'his flannel shirt', 'ancient blue hatchback', 'his tan uniform', 'was wearing black', 'wore flannel'],\n",
    "    'Colors': ['black chevy', 'a bumpy orange', 'and frosty pink', 'of cornfields amber', 'bands of lavender', 'green', 'red', 'purple'],\n",
    "\n",
    "    'Sexual_and_Romantic_Interactions': ['tried to flirt', 'me a blowjob', 'to get laid', 'satisfy me sexually', 'that my flirting', 'hitting on me', 'have sex so', 'grand romantic gesture', 'lost my virginity', 'have fucking groupies', 'a housewarming gift', 'had masturbated to', 'for hitting on', 'even had sex'],\n",
    "    'Secretive_Or_Covert_Actions': ['sneak out when', 'bribe the guards', 'of getting caught', 'to sneak out', 'sped off before', 'locked the door', 'she hung up', 'kept it hidden', 'she hanged up', \"'m not invited\", 'leaves the room', 'not tell anyone', 'could escape quietly'],\n",
    "    'Recognition': ['neighbor had recognized', 'officer heard me', 'friends saw me', 'knew exactly who', 'i provoked gasps', 'she recognized me', 'guard spotted us'],\n",
    "\n",
    "    'Positive Emotional Reactions': ['she started laughing', 'smiled and said', 'he giggled', 'they were happy to see it', 'she was relieved'],\n",
    "    'Negative Emotional Reactions': ['mom started crying', 'started to cry', 'eyed her suspiciously', 'annoyed him', 'they were upset'],\n",
    "    \"Professions\": ['his cop training', 'other egghead phds', 'parents were doctors', 'a gay speechwriter', 'a cambridge educated', 'i played college', 'been my accountant', 'my indian heritage', 'taught them japanese'],\n",
    "    \"Fear and Avoidance\": ['he screamed', 'always too scared', 'she steered clear', 'they ran away', 'constant fear', 'always afraid'],\n",
    "}\n",
    "\n",
    "ks = list(explanations_clean.keys())\n",
    "\n",
    "\n",
    "rows = {\n",
    "    # 'roi': rois,\n",
    "    'expl': [explanations_clean[k] for k in ks],\n",
    "    'top_ngrams_module_correct': [top_ngrams_clean[k] for k in ks],\n",
    "    # 'stability_score': [stability_scores[k.split('_')[0]] for k in ks],\n",
    "    'subject': [f'UT{subject}'] * len(ks),\n",
    "    # 'voxel_nums': [rois_dict[k.split('_')[0]] for k in ks],\n",
    "    'prompt_suffix': [explanation_avoid_suffixes[k] for k in ks],\n",
    "}\n",
    "rows = pd.DataFrame(rows)\n",
    "rows.to_pickle(f'rows_roi_ut{subject.lower()}_nov30.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = f'../../results/stories/roi/'\n",
    "for k in os.listdir(folder):\n",
    "    if 'nov30' in k:\n",
    "        try:\n",
    "            prompts_paragraphs = joblib.load(\n",
    "                join(folder, k, 'prompts_paragraphs.pkl'))\n",
    "            print(k, len(prompts_paragraphs['paragraphs']))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S02 Export selected rois to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = ['RSC', 'OPA', 'PPA', 'IPS', 'pSTS', 'sPMv',\n",
    "        'EBA', 'OFA'] + ['RSC_only', 'OPA_only', 'PPA_only2']  # 'PPA_only1',\n",
    "# pprint({k: explanations[k] for k in rois})\n",
    "explanations_clean = {\n",
    "    'EBA': 'Body parts',\n",
    "    'IPS': 'Descriptive elements of scenes or objects',\n",
    "    'OFA': 'Personal growth and reflection',\n",
    "    'OPA': 'Direction and location descriptions',\n",
    "    'OPA_only': 'Spatial positioning and directions',\n",
    "    'PPA': 'Scenes and settings',\n",
    "    'PPA_only': 'Unappetizing foods',\n",
    "    'RSC': 'Travel and location names',\n",
    "    'RSC_only': 'Location names',\n",
    "    'pSTS': 'Verbal interactions',\n",
    "    'sPMv': 'Time and numbers'}\n",
    "explanation_avoid_suffixes = {\n",
    "    'EBA': ' Avoid mentioning any locations.',\n",
    "    'IPS': ' Avoid mentioning any locations.',\n",
    "    'OFA': ' Avoid mentioning any locations.',\n",
    "    'OPA': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'OPA_only': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'PPA': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'PPA_only': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'RSC': '',\n",
    "    'RSC_only': '',\n",
    "    'pSTS': ' Avoid mentioning any locations.',\n",
    "    'sPMv': ' Avoid mentioning any locations.'\n",
    "}\n",
    "for roi in rois:\n",
    "    print(f'\"{roi}\":', str(\n",
    "        top_ngrams_df[roi.replace('1', '').replace('2', '')].iloc[:50].values.tolist()) + ', ')\n",
    "    # {\n",
    "    # roi:  for roi in rois\n",
    "# })\n",
    "top_ngrams_clean = {\n",
    "    \"RSC\": ['drove from vermont', 'to washington', 'in manhattan', 'here in boston', 'off into vancouver', 'moved to chicago', 'was in mexico', 'arrived in indianapolis', 'came to florida', 'i left vermont'],\n",
    "    \"OPA\": ['onto the railing', 'towards the river', 'onto the sidewalk', 'towards the doors', 'outside the windows', 'long hallway toward', 'to the horizon', 'towards the street', 'over the gulf', 'to my left', 'path that jutted', 'on the ceiling', 'on the windowsill', 'down this embankment', 'up those stairs', 'above the gulf', 'facing the beach'],\n",
    "    \"PPA\": ['mile of cornfields', 'the windowsill', 'the rolling hills', 'beautiful moonlit mountains', 'giant stone cliffs', 'a strip mall', 'nondescript office buildings', 'manicured lawns', 'lakes', 'the dark driveway', 'and shimmering skyscrapers', 'a private beach', 'the leafy garden', 'our modest backyard', 'my dorm'],\n",
    "\n",
    "    \"RSC_only\": ['florida', 'israel', 'london', 'marrakesh', 'indianapolis', 'paris', 'pennsylvania', 'tokyo', 'tenessee', 'boston', 'vermont', 'chicago', 'indianapolis'],\n",
    "    \"OPA_only\": ['towards the ceiling', 'onto the railing', 'feet hanging over', 'towards the doors', 'seats behind', 'towards the door', 'lights peeking over', 'to my left', 'situated herself behind', 'you sit backward', 'to the horizon', 'maybe twelve feet', 'at the ceiling', 'towards the street', 'of seats behind', 'twenty feet above', 'his back turned', 'see the horizon', 'seats behind the', 'to my right', 'and high rafters', 'about twenty feet', 'door behind me', 'the door behind', 'toward the back', 'over his shoulder', 'feet above the', 'hands went underneath', 'towards the ground', 'his feet hanging', 'feet touch the', 'behind her and', 'stand in front', 'down one side', 'on opposite sides', 'over the ceiling', 'on either side'],\n",
    "    # \"PPA_only\": ['kind of corny', 'his painting sucked', 'snake oil', 'liar fake', 'fake name', 'bad puns', 'as an insult', 'called baloney'],\n",
    "    \"PPA_only2\": ['like burnt steak', 'like pudding', 'tasted pretty bad', 'stale baked goods', 'the crusts', 'baloney', 'yeast extract', 'a sandwich rejected',],\n",
    "\n",
    "    \"IPS\": ['there were slats', 'four connected squares', 'in long rows', 'on the sides', 'a long narrow', 'that forms horizontal', 'long rows of', 'sixty foot wide', 'between buttered slices', 'mile thick ice', 'all four corners', 'along the top'],\n",
    "    \"pSTS\": ['said excuse me', 'says excuse me', 'room went silent', 'someone shouted', 'i provoked gasps', 'somebody then yelled', 'she started laughing', 'excuse me', 'asked i laughed', 'exhalation someone shouted', 'retorted rather loudly', 'turned and said', 'hurry she exclaimed', 'i started yelling', 'say excuse me', 'i started laughing', 'interrupted the conversation', 'breath he yelled', 'moment she gasped', 'said guess what'],\n",
    "    \"sPMv\": ['one', 'forty', 'april nineteen forty', 'was sixteen seventeen', 'five only twenty', 'three down', 'march twentieth nineteen', 'more time passed', 'fifteen meters fifty', \"turning ninety\", 'june of nineteen'],\n",
    "    \"EBA\": ['wraps his arms', 'lifted her dress', 'arms flailing', 'hands gripped the', 'grabbed her legs', 'his hands folded', 'my feet kicking', 'navigated pushy elbows', 'elbows on knees', 'over his shoulder'],\n",
    "    \"OFA\": ['of my childhood', 'newfound self esteem', 'so my shrink', 'hurtful first dates', 'recall many instances', 'it felt magical', 'answered many questions', 'my school days', 'no satisfying fantasies', 'my mom often', 'from our childhood', 'growing up we', 'good friends often', 'shaped their mind', 'everything my parents'],\n",
    "}\n",
    "\n",
    "rows = {\n",
    "    'roi': rois,\n",
    "    'expl': [explanations_clean[k] for k in rois],\n",
    "    'top_ngrams_module_correct': [top_ngrams_clean[k] for k in rois],\n",
    "    'stability_score': [stability_scores[k.split('_')[0]] for k in rois],\n",
    "    'subject': [f'UT{subject}'] * len(rois),\n",
    "    'voxel_nums': [rois_dict[k.split('_')[0]] for k in rois],\n",
    "    'prompt_suffix': [explanation_avoid_suffixes[k] for k in rois],\n",
    "}\n",
    "rows = pd.DataFrame(rows)\n",
    "rows.to_pickle(f'rows_roi_ut{subject.lower()}_may31.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
