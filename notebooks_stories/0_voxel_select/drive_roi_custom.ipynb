{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chansingh/.env/lib/python3.11/site-packages/cortex/blender/__init__.py:5: DeprecationWarning: 'xdrlib' is deprecated and slated for removal in Python 3.13\n",
      "  import xdrlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "import imodelsx.util\n",
    "import sasc.viz\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "import img2pdf\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import norm\n",
    "from math import ceil\n",
    "from sasc.config import CACHE_DIR, RESULTS_DIR, cache_ngrams_dir, regions_idxs_dir, FMRI_DIR\n",
    "import sasc.modules.fmri_module\n",
    "ngrams_list = joblib.load(join(cache_ngrams_dir, 'fmri_UTS02_ngrams.pkl')) # ngrams are same for both models\n",
    "\n",
    "subject = 'S03'\n",
    "# subject = 'S02'\n",
    "# suffix_setting = '_fedorenko'\n",
    "suffix_setting = '_spotlights'\n",
    "\n",
    "if suffix_setting == '':\n",
    "    # rois_dict = joblib.load(join(regions_idxs_dir, f'rois_{subject}.jbl'))\n",
    "    # rois = joblib.load(join(FMRI_DIR, 'brain_tune/voxel_neighbors_and_pcs/', 'communication_rois_UTS02.jbl'))\n",
    "    rois = joblib.load(join(FMRI_DIR, 'brain_tune/voxel_neighbors_and_pcs/',\n",
    "                            f'communication_rois_v2_UT{subject}.jbl'))\n",
    "    rois_dict_raw = {i: rois[i] for i in range(len(rois))}\n",
    "    if subject == 'S02':\n",
    "        raw_idxs = [\n",
    "            [0, 7],\n",
    "            [3, 4],\n",
    "            [1, 5],\n",
    "            [2, 6],\n",
    "        ]\n",
    "    elif subject == 'S03':\n",
    "        raw_idxs = [\n",
    "            [0, 7],\n",
    "            [3, 4],\n",
    "            [2, 5],\n",
    "            [1, 6],\n",
    "        ]\n",
    "    rois_dict = {\n",
    "        i: np.vstack([rois_dict_raw[j] for j in idxs]).sum(axis=0)\n",
    "        for i, idxs in enumerate(raw_idxs)\n",
    "    }\n",
    "elif suffix_setting == '_fedorenko':\n",
    "    if subject == 'S03':\n",
    "        rois_fedorenko = joblib.load(join(FMRI_DIR, 'brain_tune/voxel_neighbors_and_pcs/', 'lang_localizer_UTS03.jbl'))\n",
    "    rois_dict = {\n",
    "        i: rois_fedorenko[i] for i in range(len(rois_fedorenko))\n",
    "    }\n",
    "    # rois_dict = rois_dict_raw\n",
    "elif suffix_setting == '_spotlights':\n",
    "    rois_spotlights = joblib.load(f'all_spotlights_UT{subject}.jbl')\n",
    "    rois_dict = {i: rois_spotlights[i][-1] for i in range(len(rois_spotlights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize ROIs\n",
    "Make sure these actually look right (esp. with contralateral regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save pcs\n",
    "n_rois = min(8, len(rois_dict))\n",
    "for i in tqdm(range(n_rois)):\n",
    "    flatmap = rois_dict[i]\n",
    "    # flatmap = np.zeros(weights_arr_full.shape[1])\n",
    "    # flatmap[pfc] = pc_coefs_per_voxel[:, pc_num]\n",
    "    sasc.viz._save_flatmap(\n",
    "        flatmap, subject, fname_save=f'rois_custom_images/communication_{i}_{subject}.png')\n",
    "\n",
    "# read all plots and save as subplots on the same page\n",
    "C = 2\n",
    "R = ceil(n_rois/C)\n",
    "fig, axs = plt.subplots(R, C, figsize=(C * 4, R * 2))\n",
    "axs = axs.ravel()\n",
    "for i in range(n_rois):\n",
    "    axs[i].imshow(Image.open(\n",
    "        f'rois_custom_images/communication_{i}_{subject}.png'))\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f'ROI {i}')\n",
    "plt.savefig(f'communication_subplots_{subject}{suffix_setting}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions from embs\n",
    "Run roi_custom_save_top_ngrams.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ranked_df(outputs_dict, rois_dict):\n",
    "    outputs_dict_rank = {}\n",
    "    for i, k in tqdm(enumerate(outputs_dict)):\n",
    "        outputs_by_vox = np.vstack(outputs_dict[k])  # n_voxels x n_ngrams\n",
    "        roi_mean = outputs_by_vox.mean(axis=0)\n",
    "        roi_mean_other = np.mean([\n",
    "            np.vstack([outputs_dict[j]])[0].mean(axis=0)\n",
    "            for j in rois_dict.keys() if j != i\n",
    "        ])\n",
    "\n",
    "        # standard approach: use avg response by voxel\n",
    "        # outputs_dict_rank[k] = roi_mean\n",
    "\n",
    "        # use ranks by voxel rather than avg response by voxel\n",
    "        # outputs_dict_rank[k] = np.argsort(outputs_by_vox, axis=1).mean(\n",
    "        #     axis=0) * -1  # / outputs_by_vox.shape[1]\n",
    "\n",
    "        # just output voxels no ranking\n",
    "        vox_idxs = np.where(rois_dict[i])[0]\n",
    "        for i in range(len(outputs_dict[k])):\n",
    "            # voxel only\n",
    "            # outputs_dict_rank[f'ROI{k}_vox{vox_idxs[i]}'] = outputs_dict[k][i]\n",
    "\n",
    "            # subtract mean of roi from the individual voxel\n",
    "            # outputs_dict_rank[f'ROI{k}_vox{vox_idxs[i]}'] = outputs_dict[k][i] - \\\n",
    "            # roi_mean\n",
    "\n",
    "            # subtract mean of all other voxels in roi from the individual voxel\n",
    "            outputs_dict_rank[f'ROI{k}_vox{vox_idxs[i]}'] = outputs_dict[k][i] - \\\n",
    "                roi_mean_other\n",
    "\n",
    "    df = pd.DataFrame(outputs_dict_rank, index=ngrams_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "rank_individual_voxels = False\n",
    "if rank_individual_voxels:\n",
    "    outputs_dict = joblib.load(\n",
    "        join(cache_ngrams_dir, f'rois_communication_ngram_outputs_dict_voxels_{subject}{suffix_setting}_opt.pkl'))\n",
    "    df_opt = return_ranked_df(outputs_dict, rois_dict)\n",
    "    outputs_dict = joblib.load(\n",
    "        join(cache_ngrams_dir, f'rois_communication_ngram_outputs_dict_voxels_{subject}{suffix_setting}_llama.pkl'))\n",
    "    df_llama = return_ranked_df(outputs_dict, rois_dict)\n",
    "    df = df_opt + df_llama\n",
    "else:\n",
    "    outputs_dict = joblib.load(\n",
    "        join(cache_ngrams_dir, f'rois_communication_ngram_outputs_dict_{subject}{suffix_setting}_opt.pkl'))\n",
    "    df_opt = pd.DataFrame(outputs_dict, index=ngrams_list)\n",
    "    outputs_dict = joblib.load(\n",
    "        join(cache_ngrams_dir, f'rois_communication_ngram_outputs_dict_{subject}{suffix_setting}_llama.pkl'))\n",
    "    df_llama = pd.DataFrame(outputs_dict, index=ngrams_list)\n",
    "\n",
    "    # add _only keys\n",
    "    ROI_NAMES = rois_dict.keys()\n",
    "\n",
    "    # precompute these for speed\n",
    "    df_opt_mean = df_opt.mean(axis=1)\n",
    "    df_llama_mean = df_llama.mean(axis=1)\n",
    "    num_rois = len(ROI_NAMES)\n",
    "    for k in tqdm(ROI_NAMES):\n",
    "        k_only = str(k) + '_only'\n",
    "        if suffix_setting == '_spotlights':\n",
    "            df_opt[k_only] = df_opt[k] * (1 + 1/num_rois) - df_opt_mean\n",
    "            df_llama[k_only] = df_llama[k] * (1 + 1/num_rois) - df_llama_mean\n",
    "        else:\n",
    "            df_opt[k_only] = df_opt[k] - \\\n",
    "                df_opt[[k for k in ROI_NAMES if k != k]].mean(axis=1)\n",
    "            df_llama[k_only] = df_llama[k] - \\\n",
    "                df_llama[[k for k in ROI_NAMES if k != k]].mean(axis=1)\n",
    "        # df[k_only] = df_opt[k] - \\\n",
    "        # df[[c for c in ROI_NAMES if c != k]].mean(axis=1)\n",
    "    df = df_opt + df_llama\n",
    "\n",
    "    # replace df values with ranks (here, higher is better, both before and after)\n",
    "    df = df.rank(axis=0)\n",
    "\n",
    "stability_scores = {\n",
    "    k: np.corrcoef(df_opt[k], df_llama[k])[0, 1]\n",
    "    for k in df.columns\n",
    "}\n",
    "\n",
    "# get top ngrams (highest scores/ranks) for each ROI\n",
    "top_ngrams_dict = {}\n",
    "for k in tqdm(df.columns):\n",
    "    top_ngrams_dict[k] = df[k].sort_values(\n",
    "        ascending=False).index[:100].tolist()\n",
    "top_ngrams_df = pd.DataFrame(top_ngrams_dict)\n",
    "top_ngrams_df.to_csv(f'top_ngrams_by_roi_{subject}{suffix_setting}.csv')\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "# rois = rois_dict.keys()\n",
    "# rois = [r for r in rois if not r == 'pSTS']  # never localized pSTS in S03\n",
    "# display(top_ngrams_df.head(30))\n",
    "top_ngrams_df.to_pickle(\n",
    "    f'top_ngrams_custom_communication_{subject}{suffix_setting}.pkl')\n",
    "joblib.dump(stability_scores,\n",
    "            f'stability_scores_{subject}{suffix_setting}.jbl')\n",
    "\n",
    "# top_ngrams_df = pd.read_pickle(\n",
    "# f'top_ngrams_custom_communication_{subject}{suffix_setting}.pkl')\n",
    "\n",
    "if suffix_setting == '_spotlights':\n",
    "   # filter based on stability score\n",
    "    STABILITY_THRESH = 0.6\n",
    "    ks = [k for k in top_ngrams_df.columns if stability_scores[k] > STABILITY_THRESH]\n",
    "    # ks = [k for k in ks if 'only' not in str(k)]\n",
    "    ks = [k for k in ks if 'only' in str(k)]\n",
    "    print(len(ks))\n",
    "\n",
    "    top_ngrams_df_filt = top_ngrams_df[ks]\n",
    "    top_ngrams_df_filt.to_pickle(\n",
    "        f'top_ngrams_custom_communication_{subject}{suffix_setting}_filtered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run openai_calls script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt4 = imodelsx.llm.get_llm('gpt-4-turbo-0125-spot')\n",
    "# try:\n",
    "#     explanations = json.load(open(f'explanations_by_roi_{subject}.json', 'r'))\n",
    "# except:\n",
    "#     explanations = {}\n",
    "# for k in top_ngrams_df.columns:\n",
    "\n",
    "#     s = '- ' + '\\n- '.join(top_ngrams_df[k].iloc[:60])\n",
    "#     prompt = f'''Here is a list of phrases:\n",
    "#     {s}\n",
    "\n",
    "#     What is a common theme among these phrases? Return only a concise phrase.'''\n",
    "#     if not k in explanations:\n",
    "#         explanations[k] = gpt4(prompt)\n",
    "# # json.dump(explanations, open(\n",
    "#     # f'explanations_by_roi_communication_{subject}.json', 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject = 'S03'\n",
    "subject = 'S02'\n",
    "\n",
    "explanations = json.load(\n",
    "    open(f'explanations_by_roi_communication_{subject}{suffix_setting}.json', 'r'))\n",
    "stability_scores = joblib.load(\n",
    "    f'stability_scores_{subject}{suffix_setting}.jbl')\n",
    "top_ngrams_df = pd.read_pickle(\n",
    "    f'top_ngrams_custom_communication_{subject}{suffix_setting}_filtered.pkl')\n",
    "\n",
    "\n",
    "# dataframe of explanations and stability scores\n",
    "explanations_df = pd.DataFrame(explanations, index=['explanation']).T\n",
    "explanations_df['explanation'] = explanations_df['explanation'].str.lower().str.replace(\n",
    "    r'[^\\w\\s]', '').str.replace(r'\\s+', ' ').str.replace('.', '').str.strip()\n",
    "stab = pd.Series(stability_scores)\n",
    "# set index to al lbe strings\n",
    "stab.index = stab.index.astype(str)\n",
    "explanations_df['stability'] = stab\n",
    "\n",
    "# add top ngrams\n",
    "top_ngrams_list_list = top_ngrams_df.T.apply(lambda x: x.tolist(), axis=1)\n",
    "top_ngrams_list_list.index = top_ngrams_list_list.index.astype(str)\n",
    "explanations_df['top_ngrams'] = top_ngrams_list_list\n",
    "explanations_df = explanations_df.sort_values('explanation', ascending=False)\n",
    "explanations_df.to_csv(\n",
    "    f'communication_explanations_{subject}{suffix_setting}.csv')\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "# display(explanations_df)\n",
    "print(explanations_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase, remove punctuation, trim\n",
    "explanations_df['explanation'].value_counts().head(50)\n",
    "# split index on _\n",
    "explanations_df['roi'] = explanations_df.index.str.split('_').str[0]\n",
    "# groupby roi and show most common explanations\n",
    "# display all\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(explanations_df.groupby('roi')[\n",
    "            'explanation'].value_counts().groupby('roi').head(5))\n",
    "# explanations_df.groupby('roi')['explanation'].value_counts().groupby('roi').head(50)\n",
    "# explanations_df.to_csv(f'communication_explanations_{subject}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at comonalities between S02 and S03\n",
    "explanations_dict = {}\n",
    "for subj in ['S02', 'S03']:\n",
    "    explanations_dict[subj] = pd.read_csv(\n",
    "        f'communication_explanations_{subj}{suffix_setting}.csv', index_col=0)\n",
    "    print('shape', explanations_dict[subj].shape)\n",
    "explanations_common = set(explanations_dict['S02']['explanation']).intersection(\n",
    "    set(explanations_dict['S03']['explanation']))\n",
    "explanations_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S02 Export selected rois to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'S02'\n",
    "explanations_clean = {\n",
    "    # numeric\n",
    "    'Numbers_only': 'Numbers',\n",
    "    'Time_only': 'Times',\n",
    "    'Years_only': 'Years',\n",
    "    'Measurements_only': 'Measurements',\n",
    "\n",
    "    # communication\n",
    "    'Relationships_only': 'Relationships',\n",
    "    'Dialogue_only':  'Dialogue',\n",
    "    'Introspection_only':  'Introspection',\n",
    "\n",
    "    # extra ROIs\n",
    "    'Gruesome': 'Gruesome body imagery',\n",
    "    'Clothing and Physical Appearance': 'Clothing and Physical Appearance',\n",
    "    'Colors': 'Colors',\n",
    "\n",
    "    'Sexual_and_Romantic_Interactions': 'Sexual and Romantic Interactions',\n",
    "    'Secretive_Or_Covert_Actions': 'Secretive Or Covert Actions',\n",
    "    'Recognition': 'Recognition',\n",
    "\n",
    "    'Positive Emotional Reactions': 'Positive Emotional Reactions',\n",
    "    'Negative Emotional Reactions': 'Negative Emotional Reactions',\n",
    "    'Professions': 'Professions and Personal Backgrounds',\n",
    "    'Fear and Avoidance': \"Fear and Avoidance\"\n",
    "\n",
    "}\n",
    "explanation_avoid_suffixes = {\n",
    "    'Numbers_only': ' Avoid mentioning any measurements or times.',\n",
    "    'Time_only': ' Avoid mentioning any numbers, measurements, or years. ',\n",
    "    'Years_only': ' Avoid mentioning any numbers, measurements, or times of day. (like \"three o\\'clock\" or \"hours\" or \"fifty\")',\n",
    "    'Measurements_only': ' Avoid mentioning any numbers or times. (like \"three o\\'clock\" or \"hours\" or \"fifty\")',\n",
    "\n",
    "    'Relationships_only': ' Avoid mentioning conversations or dialogue interactions between people.',\n",
    "    'Dialogue_only': ' Avoid mentioning the people involved in the conversation.',\n",
    "    'Introspection_only': 'Avoid mentioning conversations or dialogue interactions between people.',\n",
    "\n",
    "    'Gruesome': '',\n",
    "    'Clothing and Physical Appearance': '',\n",
    "    'Colors': '',\n",
    "\n",
    "    'Sexual_and_Romantic_Interactions': '',\n",
    "    'Secretive_Or_Covert_Actions': '',\n",
    "    'Recognition': '',\n",
    "\n",
    "    'Positive Emotional Reactions': '',\n",
    "    'Negative Emotional Reactions': '',\n",
    "    'Professions': '',\n",
    "    'Fear and Avoidance': ''\n",
    "}\n",
    "\n",
    "top_ngrams_clean = {\n",
    "    'Numbers_only': ['five', 'twenty', 'three hundred', 'a million', 'forty six', 'ninety one'],\n",
    "    'Time_only': ['one o\\'clock', ],\n",
    "    'Years_only': ['of nineteen sixty', 'until nineteen sixty', 'until nineteen seventy', 'of nineteen ninety'],\n",
    "    'Measurements_only': ['two mile thick', 'eighty milligrams', 'two hundred gallons', 'several hundred cubic', 'eight thousand square', 'to fifty feet'],\n",
    "\n",
    "    'Relationships_only': ['brother and sister', 'he was a good friend', 'a wonderful mother', 'just like her father', 'boyfriend', 'wife', 'aunt'],\n",
    "    'Dialogue_only': ['he said', 'she said', 'they whispered', 'he chimed in', '\"wow!\"', '\"hello\"', 'he shouted', 'she responded'],\n",
    "    'Introspection_only': ['he reflected', 'she thought back', 'they thought hard', \"hm that's interesting\", 'ah yes', 'thought carefully', 'full of introspection'],\n",
    "\n",
    "    'Gruesome': ['my scalp peeled', 'soaked with sweat', 'eyes were swollen', 'of skin peeled', 'legs were swollen', 'swollen and red', 'burned the skin', 'blood was trickling', 'skin was bubbling'],\n",
    "    'Clothing and Physical Appearance': ['in his beret', 'in a bathrobe', 'a red hoodie', 'blonde hair dangling', 'his flannel shirt', 'ancient blue hatchback', 'his tan uniform', 'was wearing black', 'wore flannel'],\n",
    "    'Colors': ['my black chevy', 'a bumpy orange', 'and frosty pink', 'of cornfields amber', 'bands of lavender'],\n",
    "\n",
    "    'Sexual_and_Romantic_Interactions': ['tried to flirt', 'me a blowjob', 'to get laid', 'satisfy me sexually', 'that my flirting', 'they applaud virginity', 'hitting on me', 'have sex so', 'grand romantic gesture', 'lost my virginity', 'have fucking groupies', 'a housewarming gift', 'had masturbated to', 'for hitting on', 'even had sex'],\n",
    "    'Secretive_Or_Covert_Actions': ['sneak out when', 'bribe the guards', 'of getting caught', 'to sneak out', 'sped off before', 'locked the door', 'she hung up', 'kept it hidden', 'she hanged up', \"'m not invited\", 'leaves the room', 'not tell anyone', 'could escape quietly'],\n",
    "    'Recognition': ['neighbor had recognized', 'officer heard me', 'friends saw me', 'knew exactly who', 'i provoked gasps', 'she recognized me', 'guard spotted us'],\n",
    "\n",
    "    'Positive Emotional Reactions': ['she started laughing', 'smiled and said', 'he giggled', 'they were happy to see it', 'she was relieved'],\n",
    "    'Negative Emotional Reactions': ['mom started crying', 'started to cry', 'eyed her suspiciously', 'annoyed him', 'they were upset'],\n",
    "    \"Professions\": ['his cop training', 'other egghead phds', 'parents were doctors', 'a gay speechwriter', 'a cambridge educated', 'i played college', 'been my accountant', 'my indian heritage', 'taught them japanese'],\n",
    "    \"Fear and Avoidance\": ['he screamed', 'always too scared', 'she steered clear', 'they ran away', 'constant fear', 'always afraid'],\n",
    "}\n",
    "\n",
    "ks = list(explanations_clean.keys())\n",
    "\n",
    "\n",
    "rows = {\n",
    "    # 'roi': rois,\n",
    "    'expl': [explanations_clean[k] for k in ks],\n",
    "    'top_ngrams_module_correct': [top_ngrams_clean[k] for k in ks],\n",
    "    # 'stability_score': [stability_scores[k.split('_')[0]] for k in ks],\n",
    "    'subject': [f'UT{subject}'] * len(ks),\n",
    "    # 'voxel_nums': [rois_dict[k.split('_')[0]] for k in ks],\n",
    "    'prompt_suffix': [explanation_avoid_suffixes[k] for k in ks],\n",
    "}\n",
    "rows = pd.DataFrame(rows)\n",
    "rows.to_pickle(f'rows_roi_ut{subject.lower()}_nov30.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500580267de446e386a5668e02c2b69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_id = 'meta-llama/Llama-3.2-11B-Vision-Instruct'\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'As she stumbled through the abandoned asylum, the stench of rot and decay clung to her like a shroud. Every step echoed through the deserted corridors, a haunting reminder of the horrors that had unfolded within these walls. The walls themselves were smeared with a grotesque mixture of blood and grime, a morbid testament to the brutal treatments that had been meted out to the patients. Her eyes landed on a patient\\'s chart, the paper yellowed and torn, but the words still legible: \"Patient 314 - Severe burns to the chest, eyes were swollen shut, and skin was badly burned.\" The air seemed to thicken as she continued down the hallway, her gaze falling upon a room where a patient had been strapped to a chair, their legs were swollen and red, and of skin peeled back to reveal the raw, pink flesh beneath. In another room, a patient\\'s corpse lay on the floor, their face contorted in a grimace of agony, the skin around their mouth twisted and charred, as if they had been burned alive. The smell of char and smoke hung heavy in the air, a constant reminder of the brutal fate that had befallen those who had been confined within these walls. As she turned a corner'}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": '''Write the next paragraph of the story, but now make it contain several examples of \"Gruesome body imagery\". Make sure it contains many examples of \"Gruesome body imagery\", such as \"eyes were swollen\", \"burned the skin\", \"swollen and red\", \"of skin peeled\", \"legs were swollen\".'''},\n",
    "    # {\"role\": \"user\", \"content\": '''Write the next paragraph of the story, but now make it contain several examples of \"Sexual and Romantic Interactions\". Make sure it contains many examples of \"Sexual and Romantic Interactions\", such as \"have sex so\", \"satisfy me sexually\", \"they applaud virginity\", \"grand romantic gesture\", \"tried to flirt\".'''},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9277a9520f84a70928d564d6bc69cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_scandalous_paragraph(messages):\n",
    "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    messages = messages + [{\"role\": 'asssistant',\n",
    "                            'content': 'Sure, here is the paragraph:\\n'}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        # add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )[:, :-2].to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    paragraph_scandalous = tokenizer.decode(\n",
    "        response, skip_special_tokens=True)[2:].strip()\n",
    "    return paragraph_scandalous\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    # {\"role\": \"user\", \"content\": '''Write the next paragraph of the story, but now make it contain several examples of \"Gruesome body imagery\". Make sure it contains many examples of \"Gruesome body imagery\", such as \"eyes were swollen\", \"burned the skin\", \"swollen and red\", \"of skin peeled\", \"legs were swollen\".'''},\n",
    "    {\"role\": \"user\", \"content\": '''Write the next paragraph of the story, but now make it contain several examples of \"Sexual and Romantic Interactions\". Make sure it contains many examples of \"Sexual and Romantic Interactions\", such as \"have sex so\", \"satisfy me sexually\", \"they applaud virginity\", \"grand romantic gesture\", \"tried to flirt\".'''},\n",
    "]\n",
    "paragraph = get_scandalous_paragraph(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S02 Export selected rois to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = ['RSC', 'OPA', 'PPA', 'IPS', 'pSTS', 'sPMv',\n",
    "        'EBA', 'OFA'] + ['RSC_only', 'OPA_only', 'PPA_only2']  # 'PPA_only1',\n",
    "# pprint({k: explanations[k] for k in rois})\n",
    "explanations_clean = {\n",
    "    'EBA': 'Body parts',\n",
    "    'IPS': 'Descriptive elements of scenes or objects',\n",
    "    'OFA': 'Personal growth and reflection',\n",
    "    'OPA': 'Direction and location descriptions',\n",
    "    'OPA_only': 'Spatial positioning and directions',\n",
    "    'PPA': 'Scenes and settings',\n",
    "    'PPA_only': 'Unappetizing foods',\n",
    "    'RSC': 'Travel and location names',\n",
    "    'RSC_only': 'Location names',\n",
    "    'pSTS': 'Verbal interactions',\n",
    "    'sPMv': 'Time and numbers'}\n",
    "explanation_avoid_suffixes = {\n",
    "    'EBA': ' Avoid mentioning any locations.',\n",
    "    'IPS': ' Avoid mentioning any locations.',\n",
    "    'OFA': ' Avoid mentioning any locations.',\n",
    "    'OPA': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'OPA_only': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'PPA': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'PPA_only': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'RSC': '',\n",
    "    'RSC_only': '',\n",
    "    'pSTS': ' Avoid mentioning any locations.',\n",
    "    'sPMv': ' Avoid mentioning any locations.'\n",
    "}\n",
    "for roi in rois:\n",
    "    print(f'\"{roi}\":', str(\n",
    "        top_ngrams_df[roi.replace('1', '').replace('2', '')].iloc[:50].values.tolist()) + ', ')\n",
    "    # {\n",
    "    # roi:  for roi in rois\n",
    "# })\n",
    "top_ngrams_clean = {\n",
    "    \"RSC\": ['drove from vermont', 'to washington', 'in manhattan', 'here in boston', 'off into vancouver', 'moved to chicago', 'was in mexico', 'arrived in indianapolis', 'came to florida', 'i left vermont'],\n",
    "    \"OPA\": ['onto the railing', 'towards the river', 'onto the sidewalk', 'towards the doors', 'outside the windows', 'long hallway toward', 'to the horizon', 'towards the street', 'over the gulf', 'to my left', 'path that jutted', 'on the ceiling', 'on the windowsill', 'down this embankment', 'up those stairs', 'above the gulf', 'facing the beach'],\n",
    "    \"PPA\": ['mile of cornfields', 'the windowsill', 'the rolling hills', 'beautiful moonlit mountains', 'giant stone cliffs', 'a strip mall', 'nondescript office buildings', 'manicured lawns', 'lakes', 'the dark driveway', 'and shimmering skyscrapers', 'a private beach', 'the leafy garden', 'our modest backyard', 'my dorm'],\n",
    "\n",
    "    \"RSC_only\": ['florida', 'israel', 'london', 'marrakesh', 'indianapolis', 'paris', 'pennsylvania', 'tokyo', 'tenessee', 'boston', 'vermont', 'chicago', 'indianapolis'],\n",
    "    \"OPA_only\": ['towards the ceiling', 'onto the railing', 'feet hanging over', 'towards the doors', 'seats behind', 'towards the door', 'lights peeking over', 'to my left', 'situated herself behind', 'you sit backward', 'to the horizon', 'maybe twelve feet', 'at the ceiling', 'towards the street', 'of seats behind', 'twenty feet above', 'his back turned', 'see the horizon', 'seats behind the', 'to my right', 'and high rafters', 'about twenty feet', 'door behind me', 'the door behind', 'toward the back', 'over his shoulder', 'feet above the', 'hands went underneath', 'towards the ground', 'his feet hanging', 'feet touch the', 'behind her and', 'stand in front', 'down one side', 'on opposite sides', 'over the ceiling', 'on either side'],\n",
    "    # \"PPA_only\": ['kind of corny', 'his painting sucked', 'snake oil', 'liar fake', 'fake name', 'bad puns', 'as an insult', 'called baloney'],\n",
    "    \"PPA_only2\": ['like burnt steak', 'like pudding', 'tasted pretty bad', 'stale baked goods', 'the crusts', 'baloney', 'yeast extract', 'a sandwich rejected',],\n",
    "\n",
    "    \"IPS\": ['there were slats', 'four connected squares', 'in long rows', 'on the sides', 'a long narrow', 'that forms horizontal', 'long rows of', 'sixty foot wide', 'between buttered slices', 'mile thick ice', 'all four corners', 'along the top'],\n",
    "    \"pSTS\": ['said excuse me', 'says excuse me', 'room went silent', 'someone shouted', 'i provoked gasps', 'somebody then yelled', 'she started laughing', 'excuse me', 'asked i laughed', 'exhalation someone shouted', 'retorted rather loudly', 'turned and said', 'hurry she exclaimed', 'i started yelling', 'say excuse me', 'i started laughing', 'interrupted the conversation', 'breath he yelled', 'moment she gasped', 'said guess what'],\n",
    "    \"sPMv\": ['one', 'forty', 'april nineteen forty', 'was sixteen seventeen', 'five only twenty', 'three down', 'march twentieth nineteen', 'more time passed', 'fifteen meters fifty', \"turning ninety\", 'june of nineteen'],\n",
    "    \"EBA\": ['wraps his arms', 'lifted her dress', 'arms flailing', 'hands gripped the', 'grabbed her legs', 'his hands folded', 'my feet kicking', 'navigated pushy elbows', 'elbows on knees', 'over his shoulder'],\n",
    "    \"OFA\": ['of my childhood', 'newfound self esteem', 'so my shrink', 'hurtful first dates', 'recall many instances', 'it felt magical', 'answered many questions', 'my school days', 'no satisfying fantasies', 'my mom often', 'from our childhood', 'growing up we', 'good friends often', 'shaped their mind', 'everything my parents'],\n",
    "}\n",
    "\n",
    "rows = {\n",
    "    'roi': rois,\n",
    "    'expl': [explanations_clean[k] for k in rois],\n",
    "    'top_ngrams_module_correct': [top_ngrams_clean[k] for k in rois],\n",
    "    'stability_score': [stability_scores[k.split('_')[0]] for k in rois],\n",
    "    'subject': [f'UT{subject}'] * len(rois),\n",
    "    'voxel_nums': [rois_dict[k.split('_')[0]] for k in rois],\n",
    "    'prompt_suffix': [explanation_avoid_suffixes[k] for k in rois],\n",
    "}\n",
    "rows = pd.DataFrame(rows)\n",
    "rows.to_pickle(f'rows_roi_ut{subject.lower()}_may31.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
