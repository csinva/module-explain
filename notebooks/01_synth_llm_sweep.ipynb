{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import notebook_helper\n",
    "import sys\n",
    "import numpy as np\n",
    "import imodelsx\n",
    "import imodelsx.process_results\n",
    "import mprompt.data.data\n",
    "from mprompt.data.data import TASKS_D3\n",
    "# import mprompt.methods.m4_evaluate as m4_evaluate\n",
    "import warnings\n",
    "import scipy.stats\n",
    "def sem(x):\n",
    "    '''Compute standard error of the mean, ignoring NaNs\n",
    "    '''\n",
    "    with warnings.catch_warnings():\n",
    "        return scipy.stats.sem(x, ddof=0)\n",
    "tqdm.pandas()\n",
    "TASK_NAMES = list(TASKS_D3.keys())\n",
    "sys.path.append('../experiments/')\n",
    "results_dir = '/home/chansingh/mprompt/results/may10' # maps to results_synthetic_llm_sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_pickle(join(results_dir, 'results_synthetic_llm_sweep.pkl'))\n",
    "# r = imodelsx.process_results.get_results_df(results_dir, use_cached=False)\n",
    "# r = imodelsx.process_results.fill_missing_args_with_default(r, experiment_filename='01_explain.py')\n",
    "# r = notebook_helper.process_and_add_scores(r, add_bert_scores=True)\n",
    "# r.to_pickle(join(results_dir, 'results_synthetic_llm_sweep.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplifying\n",
    "default_params = {\n",
    "    'noise_ngram_scores': 0,\n",
    "    'module_num_restrict': -1,\n",
    "    'checkpoint': 'text-davinci-003',\n",
    "}\n",
    "rd = r\n",
    "for k, v in default_params.items():\n",
    "    rd = rd[rd[k] == v]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation recovery accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_3(x):\n",
    "    return x.apply(lambda x: f'{x:.3f}')\n",
    "def round_2(x):\n",
    "    return x.apply(lambda x: f'{x:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = ['noise_ngram_scores', 'ngrams_restricted']\n",
    "metrics = ['any_contains_keywords', 'top_contains_keywords', 'mean_contains_keywords']\n",
    "metrics_weighted = ['mean_contains_keywords_weighted']\n",
    "if 'score_bert' in r.columns:\n",
    "    metrics += ['any_bert', 'top_bert', 'mean_bert']\n",
    "    metrics_weighted += ['mean_bert_weighted']\n",
    "helpers = ['row_count_helper', 'num_generated_explanations']\n",
    "index_remap = {\n",
    "    (0.0, False): 'Default',\n",
    "    (0.0,  True): 'Restricted corpus',\n",
    "    (3.0, False): 'Noisy module',\n",
    "}\n",
    "\n",
    "g = (\n",
    "    r\n",
    "    .filter(groupings + metrics + metrics_weighted + helpers)\n",
    "    .groupby(groupings)\n",
    "    .aggregate(['sum', sem], numeric_only=True)\n",
    ")\n",
    "g.columns = [x[0]+'_err' if x[1] == 'sem' else x[0] for x in g.columns]\n",
    "\n",
    "for met in metrics:\n",
    "    g[met] = g[met] / g['row_count_helper']\n",
    "for met in metrics_weighted:\n",
    "    g[met] = g[met] / g['num_generated_explanations']\n",
    "\n",
    "g.index = [index_remap[x] for x in g.index.values]\n",
    "columns=[k for k in g.columns if not k.endswith('_err')]\n",
    "for col in columns:\n",
    "    err = g[col + '_err']\n",
    "    if col == 'mean_contains_keywords_weighted':\n",
    "        err = g['mean_contains_keywords_err']\n",
    "        err *= np.sqrt(g['row_count_helper'] / g['num_generated_explanations'])\n",
    "    g[col] = round_3(g[col]) + ' \\\\err{' + round_2(err) + '}'\n",
    "\n",
    "g = g.drop(columns=helpers + \n",
    "           [k for k in ['mean_contains_keywords', 'mean_bert', 'any_bert', 'any_contains_keywords'] if k in g.columns])\n",
    "\n",
    "g = g.rename(\n",
    "    columns={\n",
    "            'any_contains_keywords': 'Accuracy (top-5)',\n",
    "            'top_contains_keywords': 'Accuracy',\n",
    "            'mean_contains_keywords_weighted': '\\\\makecell{Accuracy\\\\\\\\(no ranking)}',\n",
    "            'mean_contains_keywords': 'Accuracy (no ranking, unweighted)',\n",
    "            'top_bert': 'BERT Score',\n",
    "            'mean_bert_weighted': '\\\\makecell{BERT Score\\\\\\\\(no ranking)}',\n",
    "        }\n",
    ")\n",
    "cols_print = [k for k in g.columns if not k.endswith('_err')]\n",
    "display(g[cols_print])\n",
    "print(g[cols_print].style.to_latex(hrules=True).replace('lllll', 'lcccc'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does synthetic score help explanation recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = ['noise_ngram_scores', 'ngrams_restricted', 'top_contains_keywords']\n",
    "metrics = ['top_score_synthetic']\n",
    "g2 = (\n",
    "    r\n",
    "    .filter(groupings + metrics)\n",
    "    .groupby(groupings)\n",
    "    .mean()\n",
    "    .round(3)\n",
    "    .pivot_table(index=['noise_ngram_scores', 'ngrams_restricted'],\n",
    "                 columns='top_contains_keywords',\n",
    "                 values='top_score_synthetic')\n",
    "    .rename_axis(None, axis=1)  \n",
    "    .rename(columns={\n",
    "    True: 'Synthetic score (correct)',\n",
    "    False: 'Synthetic score (incorrect)',\n",
    "    })\n",
    ")\n",
    "g2.index = [index_remap[x] for x in g2.index.values]\n",
    "display(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['task_variation'] = r.apply(lambda row: index_remap[(row.noise_ngram_scores, row.ngrams_restricted)], axis=1)\n",
    "# COLORS = ['black', 'C0', 'C1']\n",
    "COLORS = ['C0', 'C1', 'C2']\n",
    "plt.figure(figsize=(9, 4))\n",
    "# for i, task_variation in enumerate(sorted(r.task_variation.unique())):\n",
    "for i, task_variation in enumerate(['Default', 'Restricted corpus', 'Noisy module']):\n",
    "    d = r[r.task_variation == task_variation]\n",
    "\n",
    "\n",
    "    # compute corrects\n",
    "    scores_synth = sum(d.score_synthetic.values, [])\n",
    "    corrects = sum(d.score_contains_keywords.values, [])\n",
    "    args = np.argsort(scores_synth)\n",
    "    scores_synth = np.array(scores_synth)[args]\n",
    "    corrects = np.array(corrects)[args]\n",
    "\n",
    "    # plot\n",
    "    cum = np.arange(1, len(corrects) + 1)\n",
    "    cumsums = np.cumsum(corrects) / cum\n",
    "    sems = np.sqrt(cumsums * (1 - cumsums) / cum)\n",
    "\n",
    "    # only plot >= 0\n",
    "    args2 = scores_synth >= -1000\n",
    "    s = scores_synth[args2]\n",
    "    c = cumsums[args2]\n",
    "    sems = sems[args2]\n",
    "\n",
    "    x = s\n",
    "    x = np.arange(len(s)) / len(s) * 100\n",
    "    plt.plot(x, c, '-', label=task_variation, lw=3, color=COLORS[i])\n",
    "    plt.fill_between(x, c - sems, c + sems, alpha=0.2, interpolate=True, color=COLORS[i])\n",
    "    plt.ylabel('Cumulative accuracy')\n",
    "    plt.xlabel('Explanation score (percentile)')\n",
    "    plt.grid()\n",
    "    # plt.show()\n",
    "\n",
    "plt.legend(labelcolor='linecolor', bbox_to_anchor=(1.05, 1), loc='upper left', title='Setting')\n",
    "plt.xlim(10, 100)\n",
    "plt.ylim(0.0, 0.8)\n",
    "# dvu.line_legend()\n",
    "plt.savefig('../results/synthetic/explanation_score_curves.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
