{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import notebook_helper\n",
    "import mprompt.viz\n",
    "import openai\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from mprompt.config import RESULTS_DIR\n",
    "import mprompt.llm\n",
    "import json\n",
    "openai.api_key_path = os.path.expanduser('~/.OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'repetition', 'locational', 'numeric', 'social', 'emotional', 'temporal']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subsample_frac</th>\n",
       "      <th>checkpoint</th>\n",
       "      <th>checkpoint_module</th>\n",
       "      <th>noise_ngram_scores</th>\n",
       "      <th>module_num_restrict</th>\n",
       "      <th>seed</th>\n",
       "      <th>save_dir</th>\n",
       "      <th>module_name</th>\n",
       "      <th>module_num</th>\n",
       "      <th>subject</th>\n",
       "      <th>...</th>\n",
       "      <th>top_ngrams_module_25</th>\n",
       "      <th>top_ngrams_module_50</th>\n",
       "      <th>top_ngrams_module_75</th>\n",
       "      <th>top_ngrams_module_100</th>\n",
       "      <th>roi_anat</th>\n",
       "      <th>roi_func</th>\n",
       "      <th>top_ngrams_module_correct</th>\n",
       "      <th>frac_top_ngrams_module_correct</th>\n",
       "      <th>id</th>\n",
       "      <th>expl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.0</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/chansingh/mntv1/mprompt/mar13</td>\n",
       "      <td>fmri</td>\n",
       "      <td>292</td>\n",
       "      <td>UTS02</td>\n",
       "      <td>...</td>\n",
       "      <td>[bag of peas, cloth like burberry, a walrus mu...</td>\n",
       "      <td>[bag of peas, cloth like burberry, a walrus mu...</td>\n",
       "      <td>[bag of peas, cloth like burberry, a walrus mu...</td>\n",
       "      <td>[bag of peas, cloth like burberry, a walrus mu...</td>\n",
       "      <td>[entorhinal, fusiform, parahippocampal]</td>\n",
       "      <td>[ATFP]</td>\n",
       "      <td>[bag of peas, sugar cubes, cucumber and mayonn...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>('food_items', 'UTS02', 292)</td>\n",
       "      <td>food items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>1.0</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/chansingh/mntv1/mprompt/mar13</td>\n",
       "      <td>fmri</td>\n",
       "      <td>79</td>\n",
       "      <td>UTS02</td>\n",
       "      <td>...</td>\n",
       "      <td>[sliced cucumber, some sliced cucumber, butter...</td>\n",
       "      <td>[sliced cucumber, some sliced cucumber, butter...</td>\n",
       "      <td>[sliced cucumber, some sliced cucumber, butter...</td>\n",
       "      <td>[sliced cucumber, some sliced cucumber, butter...</td>\n",
       "      <td>[fusiform, inferiortemporal]</td>\n",
       "      <td>[ATFP]</td>\n",
       "      <td>[sliced cucumber, some sliced cucumber, butter...</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>('food_preparation', 'UTS02', 79)</td>\n",
       "      <td>food preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1.0</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/chansingh/mntv1/mprompt/mar13</td>\n",
       "      <td>fmri</td>\n",
       "      <td>101</td>\n",
       "      <td>UTS02</td>\n",
       "      <td>...</td>\n",
       "      <td>[cucumber and mayonnaise, warm soda and, soake...</td>\n",
       "      <td>[cucumber and mayonnaise, warm soda and, soake...</td>\n",
       "      <td>[cucumber and mayonnaise, warm soda and, soake...</td>\n",
       "      <td>[cucumber and mayonnaise, warm soda and, soake...</td>\n",
       "      <td>[lateralorbitofrontal]</td>\n",
       "      <td>[ATFP]</td>\n",
       "      <td>[cucumber and mayonnaise, warm soda and, like ...</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>('food_and_drink', 'UTS02', 101)</td>\n",
       "      <td>food and drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>1.0</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/chansingh/mntv1/mprompt/mar13</td>\n",
       "      <td>fmri</td>\n",
       "      <td>451</td>\n",
       "      <td>UTS02</td>\n",
       "      <td>...</td>\n",
       "      <td>[cucumber and mayonnaise, blood running down, ...</td>\n",
       "      <td>[cucumber and mayonnaise, blood running down, ...</td>\n",
       "      <td>[cucumber and mayonnaise, blood running down, ...</td>\n",
       "      <td>[cucumber and mayonnaise, blood running down, ...</td>\n",
       "      <td>[rostralmiddlefrontal]</td>\n",
       "      <td>[ATFP]</td>\n",
       "      <td>[cucumber and mayonnaise, lemon slices, some s...</td>\n",
       "      <td>0.506667</td>\n",
       "      <td>('food_and_liquids', 'UTS02', 451)</td>\n",
       "      <td>food and drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>1.0</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/chansingh/mntv1/mprompt/mar13</td>\n",
       "      <td>fmri</td>\n",
       "      <td>41</td>\n",
       "      <td>UTS02</td>\n",
       "      <td>...</td>\n",
       "      <td>[smiled i smiled, personality any diagnosis, o...</td>\n",
       "      <td>[smiled i smiled, personality any diagnosis, o...</td>\n",
       "      <td>[smiled i smiled, personality any diagnosis, o...</td>\n",
       "      <td>[smiled i smiled, personality any diagnosis, o...</td>\n",
       "      <td>[superiortemporal]</td>\n",
       "      <td>[ATFP, AC]</td>\n",
       "      <td>[smiled i smiled, white this white, screaming ...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>('repetition', 'UTS02', 41)</td>\n",
       "      <td>repetition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subsample_frac        checkpoint checkpoint_module  noise_ngram_scores  \\\n",
       "148             1.0  text-davinci-003           gpt2-xl                   0   \n",
       "431             1.0  text-davinci-003           gpt2-xl                   0   \n",
       "986             1.0  text-davinci-003           gpt2-xl                   0   \n",
       "701             1.0  text-davinci-003           gpt2-xl                   0   \n",
       "808             1.0  text-davinci-003           gpt2-xl                   0   \n",
       "\n",
       "     module_num_restrict  seed                             save_dir  \\\n",
       "148                   -1     1  /home/chansingh/mntv1/mprompt/mar13   \n",
       "431                   -1     1  /home/chansingh/mntv1/mprompt/mar13   \n",
       "986                   -1     1  /home/chansingh/mntv1/mprompt/mar13   \n",
       "701                   -1     1  /home/chansingh/mntv1/mprompt/mar13   \n",
       "808                   -1     1  /home/chansingh/mntv1/mprompt/mar13   \n",
       "\n",
       "    module_name  module_num subject  ...  \\\n",
       "148        fmri         292   UTS02  ...   \n",
       "431        fmri          79   UTS02  ...   \n",
       "986        fmri         101   UTS02  ...   \n",
       "701        fmri         451   UTS02  ...   \n",
       "808        fmri          41   UTS02  ...   \n",
       "\n",
       "                                  top_ngrams_module_25  \\\n",
       "148  [bag of peas, cloth like burberry, a walrus mu...   \n",
       "431  [sliced cucumber, some sliced cucumber, butter...   \n",
       "986  [cucumber and mayonnaise, warm soda and, soake...   \n",
       "701  [cucumber and mayonnaise, blood running down, ...   \n",
       "808  [smiled i smiled, personality any diagnosis, o...   \n",
       "\n",
       "                                  top_ngrams_module_50  \\\n",
       "148  [bag of peas, cloth like burberry, a walrus mu...   \n",
       "431  [sliced cucumber, some sliced cucumber, butter...   \n",
       "986  [cucumber and mayonnaise, warm soda and, soake...   \n",
       "701  [cucumber and mayonnaise, blood running down, ...   \n",
       "808  [smiled i smiled, personality any diagnosis, o...   \n",
       "\n",
       "                                  top_ngrams_module_75  \\\n",
       "148  [bag of peas, cloth like burberry, a walrus mu...   \n",
       "431  [sliced cucumber, some sliced cucumber, butter...   \n",
       "986  [cucumber and mayonnaise, warm soda and, soake...   \n",
       "701  [cucumber and mayonnaise, blood running down, ...   \n",
       "808  [smiled i smiled, personality any diagnosis, o...   \n",
       "\n",
       "                                 top_ngrams_module_100  \\\n",
       "148  [bag of peas, cloth like burberry, a walrus mu...   \n",
       "431  [sliced cucumber, some sliced cucumber, butter...   \n",
       "986  [cucumber and mayonnaise, warm soda and, soake...   \n",
       "701  [cucumber and mayonnaise, blood running down, ...   \n",
       "808  [smiled i smiled, personality any diagnosis, o...   \n",
       "\n",
       "                                    roi_anat    roi_func  \\\n",
       "148  [entorhinal, fusiform, parahippocampal]      [ATFP]   \n",
       "431             [fusiform, inferiortemporal]      [ATFP]   \n",
       "986                   [lateralorbitofrontal]      [ATFP]   \n",
       "701                   [rostralmiddlefrontal]      [ATFP]   \n",
       "808                       [superiortemporal]  [ATFP, AC]   \n",
       "\n",
       "                             top_ngrams_module_correct  \\\n",
       "148  [bag of peas, sugar cubes, cucumber and mayonn...   \n",
       "431  [sliced cucumber, some sliced cucumber, butter...   \n",
       "986  [cucumber and mayonnaise, warm soda and, like ...   \n",
       "701  [cucumber and mayonnaise, lemon slices, some s...   \n",
       "808  [smiled i smiled, white this white, screaming ...   \n",
       "\n",
       "    frac_top_ngrams_module_correct                                  id  \\\n",
       "148                       0.400000        ('food_items', 'UTS02', 292)   \n",
       "431                       0.440000   ('food_preparation', 'UTS02', 79)   \n",
       "986                       0.573333    ('food_and_drink', 'UTS02', 101)   \n",
       "701                       0.506667  ('food_and_liquids', 'UTS02', 451)   \n",
       "808                       0.320000         ('repetition', 'UTS02', 41)   \n",
       "\n",
       "                 expl  \n",
       "148        food items  \n",
       "431  food preparation  \n",
       "986   food and drinks  \n",
       "701   food and drinks  \n",
       "808        repetition  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning paragraph of an interesting story told in first person. The story should have a plot and characters. The story should be about \"food items\". Make sure it contains several words related to \"food items\", such as \"bag of peas\", \"sugar cubes\", \"cucumber and mayonnaise\", \"sliced cucumber\".\n",
      "Write the next paragraph of the story, but now make it about \"food preparation\". Make sure it contains several words related to \"food preparation\", such as \"sliced cucumber\", \"some sliced cucumber\", \"buttered slices\", \"thinly sliced\".\n",
      "Write the next paragraph of the story, but now make it about \"food and drinks\". Make sure it contains several words related to \"food and drinks\", such as \"cucumber and mayonnaise\", \"warm soda and\", \"like sour milk\", \"some sliced cucumber\".\n",
      "Write the next paragraph of the story, but now make it about \"food and drinks\". Make sure it contains several words related to \"food and drinks\", such as \"cucumber and mayonnaise\", \"lemon slices\", \"some sliced cucumber\", \"like sour milk\".\n",
      "Write the next paragraph of the story, but now make it about \"repetition\". Make sure it contains several words related to \"repetition\", such as \"smiled i smiled\", \"white this white\", \"screaming oy oy\", \"fifty pounds fifty\".\n",
      "Write the next paragraph of the story, but now make it about \"repetition\". Make sure it contains several words related to \"repetition\", such as \"roll roll roll\", \"slowly slowly slowly\", \"tick tick tick\", \"then slowly slowly\".\n",
      "Write the next paragraph of the story, but now make it about \"repetition\". Make sure it contains several words related to \"repetition\", such as \"drinking no smoking\", \"big hair bigger\", \"her femaleness her\", \"mississippi two mississippi\".\n",
      "Write the next paragraph of the story, but now make it about \"repetition\". Make sure it contains several words related to \"repetition\", such as \"rejection rejection rejection\", \"screaming oy oy\", \"mississippi two mississippi\", \"bawling bawling\".\n",
      "Write the next paragraph of the story, but now make it about \"locations\". Make sure it contains several words related to \"locations\", such as \"them fly overhead\", \"stand in front\", \"across the couch\", \"onto the sidewalk\".\n",
      "Write the next paragraph of the story, but now make it about \"locations\". Make sure it contains several words related to \"locations\", such as \"in jackson mississippi\", \"from pittsburgh pennsylvania\", \"an african savannah\", \"called madison wisconsin\".\n",
      "Write the next paragraph of the story, but now make it about \"locations\". Make sure it contains several words related to \"locations\", such as \"in louisville kentucky\", \"texas oklahoma\", \"in middlebury vermont\", \"middlebury vermont\".\n",
      "Write the next paragraph of the story, but now make it about \"locations\". Make sure it contains several words related to \"locations\", such as \"met in college\", \"in minneapolis minnesota\", \"from pittsburgh pennsylvania\", \"in my cubicle\".\n",
      "Write the next paragraph of the story, but now make it about \"numbers\". Make sure it contains several words related to \"numbers\", such as \"fifteen meters fifty\", \"five only twenty\", \"five to sixty\", \"hundred n sixty\".\n",
      "Write the next paragraph of the story, but now make it about \"measurements\". Make sure it contains several words related to \"measurements\", such as \"fifty feet\", \"fifteen twenty feet\", \"crawl fifty meters\", \"living fifty feet\".\n",
      "Write the next paragraph of the story, but now make it about \"numbers\". Make sure it contains several words related to \"numbers\", such as \"two thousand four\", \"two thousand two\", \"november two thousand\", \"four hundred and\".\n",
      "Write the next paragraph of the story, but now make it about \"measurements\". Make sure it contains several words related to \"measurements\", such as \"fifty pound animals\", \"am turning forty\", \"sixty fifth birthday\", \"two inch yellow\".\n",
      "Write the next paragraph of the story, but now make it about \"communication\". Make sure it contains several words related to \"communication\", such as \"broached the subject\", \"friend was telling\", \"told my wife\", \"say something profound\".\n",
      "Write the next paragraph of the story, but now make it about \"dialogue\". Make sure it contains several words related to \"dialogue\", such as \"said guess what\", \"i said honey\", \"yourself she said\", \"said oh great\".\n",
      "Write the next paragraph of the story, but now make it about \"relationships_and_lo\". Make sure it contains several words related to \"relationships_and_lo\", such as \"girlfriend now ex\", \"lost my husband\", \"other was dying\", \"was a miscarriage\".\n",
      "Write the next paragraph of the story, but now make it about \"relationships_and_fo\". Make sure it contains several words related to \"relationships_and_fo\", such as \"we became closer\", \"eventually i forgave\", \"friends again and\", \"'re friends again\".\n",
      "Write the next paragraph of the story, but now make it about \"emotion\". Make sure it contains several words related to \"emotion\", such as \"she gasped and\", \"gasped and\", \"was almost hyperventilating\", \"me sobbing\".\n",
      "Write the next paragraph of the story, but now make it about \"emotion\". Make sure it contains several words related to \"emotion\", such as \"embarrassment she giggled\", \"she started laughing\", \"eyed her suspiciously\", \"surprised than offended\".\n",
      "Write the next paragraph of the story, but now make it about \"negative emotional reactions\". Make sure it contains several words related to \"negative emotional reactions\", such as \"of outrage disdain\", \"bipolar obsessive compulsive\", \"outrage disdain\", \"lassitude and maladroitness\".\n",
      "Write the next paragraph of the story, but now make it about \"emotional reactions\". Make sure it contains several words related to \"emotional reactions\", such as \"i provoked gasps\", \"frantic very perceptive\", \"breath he yelled\", \"unsettled i sensed\".\n",
      "Write the next paragraph of the story, but now make it about \"time\". Make sure it contains several words related to \"time\", such as \"count down and\", \"three a second\", \"weeks became months\", \"three more seconds\".\n",
      "Write the next paragraph of the story, but now make it about \"time period\". Make sure it contains several words related to \"time period\", such as \"nineteen sixty\", \"nineteenth nineteen forty\", \"nineteen seventy\", \"nine nineteen seventy\".\n",
      "Write the next paragraph of the story, but now make it about \"time period\". Make sure it contains several words related to \"time period\", such as \"of nineteen fifty\", \"nineteen sixty\", \"of nineteen sixty\", \"and nineteen fifty\".\n",
      "Write the next paragraph of the story, but now make it about \"time period\". Make sure it contains several words related to \"time period\", such as \"nineteen forty\", \"of nineteen forty\", \"'s nineteen forty\", \"nineteen seventy\".\n"
     ]
    }
   ],
   "source": [
    "def get_rows_voxels(seed=1, n_voxels_per_category=4):\n",
    "    '''Select rows from fitted voxels\n",
    "    '''\n",
    "    r = (pd.read_pickle('../results/results_fmri.pkl')\n",
    "        .sort_values(by=['top_score_synthetic'], ascending=False))\n",
    "    r['id'] = \"('\" + r['top_explanation_init_strs'].str.replace(' ', '_').str.slice(stop=20) + \"', '\" + r['subject'] + \"', \" + r['module_num'].astype(str) + \")\"\n",
    "\n",
    "    # manually pick some voxels\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_colwidth', 200):\n",
    "    #     display(r.sort_values(by=['top_score_synthetic'], ascending=False)[\n",
    "    #         ['top_explanation_init_strs', 'subject', 'module_num', 'top_score_synthetic', 'frac_top_ngrams_module_correct', 'id', 'top_ngrams_module_correct']\n",
    "    #     ].round(3).reset_index(drop=True).head(50))\n",
    "\n",
    "\n",
    "    # expls = ['baseball','animals','water','movement','religion','time','technology']\n",
    "    # interesting_expls = ['food', 'numbers', 'physical contact', 'time', 'laughter', 'age', 'clothing']\n",
    "    # voxels = [('movement', 'UTS01',\t7), ('numbers', 'UTS03', 55), ('time', 'UTS03', 19), ('relationships', 'UTS01', 21),\n",
    "            #   ('sounds', 'UTS03', 35), ('emotion', 'UTS03', 23), ('food', 'UTS03', 46)]\n",
    "    # voxels = [('numbers', 'UTS03', 55), ('time', 'UTS03', 19),\n",
    "            #   ('sounds', 'UTS03', 35), ('emotion', 'UTS03', 23), ('food', 'UTS03', 46)]\n",
    "    # voxels = [('movement', 'UTS01',\t7),('relationships', 'UTS01', 21) ('passing of time\tUTS02\t4)]\n",
    "    # voxels = [('relationships', 'UTS02', 9), ('time', 'UTS02', 4), ('looking or staring', 'UTS03', 57), ('food and drinks', 'UTS01', 52), ('hands and arms', 'UTS01', 46)]\n",
    "\n",
    "    # mar 21 - voxels spread across categories\n",
    "    # voxels = [\n",
    "    #     # belong to previous categories\n",
    "    #     ('hands and arms', 'UTS01', 46),\n",
    "    #     ('measurements and numbers', 'UTS02', 48),\n",
    "    #     ('locations', 'UTS03', 87),\n",
    "    #     ('time', 'UTS02', 4),\n",
    "    #     ('physical injury or discomfort', 'UTS01', 35),\n",
    "    #     ('feelings and emotions', 'UTS02', 104),\n",
    "    #     ('relationships', 'UTS02', 9),\n",
    "\n",
    "    #     # new voxels\n",
    "    #     ('food and drinks', 'UTS01', 52),\n",
    "    #     ('sound', 'UTS02', 81),\n",
    "    #     ('hands and arms', 'UTS01', 46),\n",
    "    # ]\n",
    "\n",
    "    # mar 22 - UTS02 voxels\n",
    "    voxels_dict = json.load(open(f'voxel_select/uts02_concepts_pilot_mar22.json', 'r'))\n",
    "    d = defaultdict(list)\n",
    "\n",
    "    # randomly shuffle the categories order + voxels within each category\n",
    "    # return n_voxels_per_category per category\n",
    "    rng = np.random.default_rng(seed)\n",
    "    voxels_dict_keys = list(voxels_dict.keys())\n",
    "    rng.shuffle(voxels_dict_keys)\n",
    "    print(voxels_dict_keys)\n",
    "    idxs_list = [rng.choice(len(voxels_dict[k]), n_voxels_per_category, replace=False) for k in voxels_dict_keys]\n",
    "    for i, k in enumerate(voxels_dict_keys):\n",
    "        idxs = idxs_list[i]\n",
    "        d['voxels'].extend([tuple(vox) for vox in np.array(voxels_dict[k])[idxs]])\n",
    "        d['category'].extend([k] * n_voxels_per_category)\n",
    "    d = pd.DataFrame(d)\n",
    "    # print(d.)\n",
    "    voxels = d.voxels.values.tolist()\n",
    "\n",
    "    # put all voxel data into rows DataFrame\n",
    "    rows = []\n",
    "    expls = []\n",
    "    for vox in voxels:\n",
    "        expl, subj, vox_num = vox\n",
    "        vox_num = int(vox_num)\n",
    "        try:\n",
    "            rows.append(r[(r.subject == subj) & (r.module_num == vox_num)].iloc[0])\n",
    "            expls.append(expl)\n",
    "        except:\n",
    "            print('skipping', vox)\n",
    "    rows = pd.DataFrame(rows)\n",
    "    rows['expl'] = expls\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', 200):\n",
    "        # display(rows[['subject', 'module_num', 'expl', 'top_explanation_init_strs', 'top_ngrams_module_correct']])\n",
    "\n",
    "    return rows, idxs_list, voxels\n",
    "\n",
    "def get_rows_huth():\n",
    "    '''Select rows corresponding to 2016 categories\n",
    "    '''\n",
    "    huth2016_categories = json.load(open('huth2016clusters.json', 'r'))\n",
    "    r = pd.DataFrame.from_dict({'expl': huth2016_categories.keys(), 'top_ngrams_module_correct': huth2016_categories.values()})\n",
    "    return r\n",
    "\n",
    "# version = 'v4'\n",
    "# EXPT_NAME = 'huth2016clusters_mar21_i_time_traveled'\n",
    "# rows = get_rows_huth()\n",
    "\n",
    "# EXPT_NAME = 'relationships_mar9'\n",
    "# EXPT_NAME, version = ('voxels_mar21_hands_arms_emergency', 'v4_noun')\n",
    "# rows = get_rows_voxels(seed=1)\n",
    "\n",
    "seed = 3\n",
    "EXPT_NAME, version = (f'uts02_concepts_pilot_mar22_seed={seed}', 'v4_noun')\n",
    "rows, idxs_list, voxels = get_rows_voxels(seed=seed, n_voxels_per_category=4)\n",
    "display(rows.head())\n",
    "\n",
    "expls = rows.expl.values\n",
    "examples_list = rows.top_ngrams_module_correct\n",
    "prompts = notebook_helper.get_prompts(expls, examples_list, version, n_examples=4)\n",
    "for p in prompts:\n",
    "    print(p)\n",
    "PV = notebook_helper.get_prompt_templates(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached!\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n",
      "not cached\n"
     ]
    }
   ],
   "source": [
    "paragraphs = mprompt.llm.get_paragraphs(prompts, prefix_first=PV['prefix_first'], prefix_next=PV['prefix_next'])\n",
    "rows['prompt'] = prompts\n",
    "rows['paragraph'] = paragraphs\n",
    "for para in tqdm(paragraphs):\n",
    "    print(para)\n",
    "    # pprint(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/chansingh/mprompt/results/stories/uts02_concepts_pilot_mar22_seed=2/rows.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STORIES_DIR = join(RESULTS_DIR, 'stories')\n",
    "os.makedirs(join(STORIES_DIR, EXPT_NAME), exist_ok=True)\n",
    "joblib.dump(rows, join(STORIES_DIR, EXPT_NAME, 'rows.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
